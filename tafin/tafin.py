#####################################
#   modules
#####################################

from _preproc import train_test
import pycrfsuite
import numpy as np
from sklearn.metrics import classification_report

"""
Remember, the format of the train & test data looks like this:
    [('Ich', 'PPER', 'O'), ('hab', 'VAFIN', 'O'), ('dann', 'ADV', 'O'), 
    ('auch', 'ADV', 'O'), ('schnell', 'ADJD', 'O'), ('gewählt', 'VVPP', 'O'), 
    ('und', 'KON', 'O'), ('saß', 'VVFIN', 'O'), ('mit', 'APPR', 'O'), 
    ('meiner', 'PPOSAT', 'O'), ('sehr', 'PTKIFG', 'B-ITSF'), 
    ('aufgeräumten', 'ADJA', 'O'), (',', '$,', 'O'), ('gut', 'ADJD', 'O'), 
    ('gelaunten', 'ADJA', 'O'), ('und', 'KON', 'O'), ('gesprächigen', 'ADJA', 'O'), 
    ('Tochter', 'NN', 'O'), ('beim', 'APPRART', 'O'), ('Essen', 'NN', 'O'), 
    ('.', '$.', 'O')], [('WIR', 'PPER', 'O'), ('SIND', 'VAFIN', 'O'), 
    ('ZUSAMMEN', 'ADV', 'O'), ('ESSEN', 'NN', 'O'), ('GEGANGEN', 'VVPP', 'O'), 
    ('.', '$.', 'O')], [('In', 'APPR', 'O'), ('einem', 'ART', 'O'), 
    ('richtigen', 'ADJA', 'O'), ('Restaurant', 'NN', 'O'), (',', '$,', 'O'), 
    ('Freitagsabends', 'NN', 'O'), ('.', '$.', 'O')]]
"""

########## here starts the model ##########

#####################################
#   functions
#####################################

def wordfeatures(doc, i):
    """
        Function that generates the local word features for 
        every token in the train/test data.
        Input:
        1. doc (list)       :   One sentence of all the sentences 
                                which contains each token as an 
                                element.
        2. i (int)          :   Index of each token.
        Output:
        1. features (dict)  :   Dictionary, that contains all the
                                features generated by the function.
                                Keys are the names of the features. 
                                Values are the corresponding 'answers'
                                of what the key asks for.
    """
    # extract token and pos-tag of token on index i
    token = doc[i][0]
    postag = doc[i][1]

    # features to generate for every token 
    features = {
        "bias": 1.0,
        "token": token,
        "token.islower()": token.islower(), 
        "postag": postag,
    }

    # features to extract for every token, starting from 
    # the second token in the sentence
    if i > 0:
        prev_token = doc[i-1][0]
        prev_postag = doc[i-1][1]
        features.update({
            "prev_token": prev_token,
            "prev_postag": prev_postag,
            "prev_token.islower()": prev_token.islower(), 
            "prev_label": doc[i-1][2]
        })

    # if i=0 token is at the beginning of a sentence
    else:
        features.update({"bos": True})
        
    # features to extract for every token except the last one
    if i < len(doc)-1:
        next_token = doc[i+1][0]
        next_postag = doc[i+1][1]
        features.update({
            "next_token": next_token,
            "next_token.islower()": next_token.islower(),
            "next_postag": next_postag,
        })

    # if i=len(doc)-1, it is the last token of the sentence and
    # therefore the end of a sentence
    else:
        features.update({"eos": True})

    return features

# function for extracting features in documents
def extract_features(doc):
    """
        Extracts the features for all the token in a
        sentence/document.
        Input:
        1. doc (list)   :   One sentence of all the sentences 
                            which contains each token as an 
                            element.
        2. i (int)      :   Index of each token.
        Output:
        features (dict) :   Dictionary, that contains all the
                            features generated by the function.
                            Keys are the names of the features. 
                            Values are the corresponding 'answers'
                            of what the key asks for.
    """
    return (wordfeatures(doc, i) for i in range(len(doc)))

# function fo generating the list of labels for each document
def get_labels(doc):
    """
        Generates a list of all the labels for each token in the
        document/sentence.
        Input:
        1. doc (list)   :   One sentence of all the sentences 
                            which contains each token as an 
                            element.
        Output:
        1. label (str)  :   Label of the token.
    """
    return [label for (token, postag, label) in doc]
    

# generate the train and test data
training_data, test_data = train_test()

# list for inputs of train data
x_train = list()
for doc in training_data:
    x_train.append(extract_features(doc))

# list for inputs of test data
x_test = list()
for doc in test_data:
    x_test.append(extract_features(doc))

# list of labels of train data
y_train = [get_labels(doc) for doc in training_data]

# list of labels of test data
y_test = [get_labels(doc) for doc in test_data]

"""
    x_train for one doc/sentence looks like this:
    [{'bias': 1.0, 'word': 'Es', 'word.islower()': False, 'postag': 'PPER', 'next_token': 'ist', 'next_token.islower()': True, 'next_postag': 'VAFIN'}, 
    {'bias': 1.0, 'word': 'ist', 'word.islower()': True, 'postag': 'VAFIN', 'prev_token': 'Es', 'prev_postag': 'PPER', 'next_token': 'sau', 'next_token.islower()': True, 'next_postag': 'ITJ'}, 
    {'bias': 1.0, 'word': 'sau', 'word.islower()': True, 'postag': 'ITJ', 'prev_token': 'ist', 'prev_postag': 'VAFIN', 'next_token': 'kalt', 'next_token.islower()': True, 'next_postag': 'ADJD'}, 
    {'bias': 1.0, 'word': 'kalt', 'word.islower()': True, 'postag': 'ADJD', 'prev_token': 'sau', 'prev_postag': 'ITJ', 'next_token': '.', 'next_token.islower()': False, 'next_postag': '$.'}, 
    {'bias': 1.0, 'word': '.', 'word.islower()': False, 'postag': '$.', 'prev_token': 'kalt', 'prev_postag': 'ADJD'}
    ]]
"""

# training the model
trainer = pycrfsuite.Trainer(verbose=True)

# for each sequence of input and output within the input and output data
for xseq, yseq in zip(x_train, y_train):
    # train the model
    trainer.append(xseq, yseq)

# paramenters of the model
trainer.set_params({
    # coefficient for L1 penalty
    'c1': 0.5,

    # coefficient for L2 penalty
    'c2': 0.01,  

    # maximum number of iterations
    'max_iterations': 200,

    # whether to include transitions that
    # are possible, but not observed
    'feature.possible_transitions': True
    })


# filename as a parameter for the train function
# trained model will be saved within the file
trainer.train("crf.model")

# load and use the trained model
tagger = pycrfsuite.Tagger()
tagger.open("crf.model")
y_pred = [tagger.tag(xseq) for xseq in x_test]

